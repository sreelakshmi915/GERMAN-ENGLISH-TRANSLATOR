# -*- coding: utf-8 -*-
"""GERMAN_ENGLISH_TRANSLATOR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YtnTXqjbX35zpCzUmFCCY7f710Hs7iFf
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing modules
import string
import re
from numpy import array, argmax, random, take
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, RepeatVector
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint
from keras.preprocessing.sequence import pad_sequences
from keras.models import load_model
from keras import optimizers
import matplotlib.pyplot as plt
# %matplotlib inline
pd.set_option('display.max_colwidth', 200)

#to read
def read_data(filename):
    file= open(filename,mode="rt",encoding='UTF-8')#TO OPEN
    text = file.read()
    file.close()
    return text

def to_lines(text):
      sents = text.strip().split('\n') #formed sentences
      sents = [i.split('\t') for i in sents]
      return sents

file = read_data("/content/drive/MyDrive/GERMAN_ENGLISH_TRANSLATOR/deu.txt")
ger_eng = to_lines(file)
ger_eng=array(ger_eng)#converted to array

ger_eng = ger_eng[:50000,:2]#we ned 1st 5000 rows
print(ger_eng)

#Text Pre-Processing
#cleaning
# Remove punctuation
ger_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ger_eng[:,0]]
ger_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in ger_eng[:,1]]

print(ger_eng)

for i in range(len(ger_eng)):
    ger_eng[i,0] = ger_eng[i,0].lower()
    ger_eng[i,1] = ger_eng[i,1].lower()

# Text to Sequence Conversion


ger=[]
eng =[]
for i in ger_eng[:,0]:
      eng.append(len(i.split()))

for i in ger_eng[:,1]:
      ger.append(len(i.split()))

length_df = pd.DataFrame({'eng':eng, 'ger':ger}) #converting to pd dataframe

length_df.hist(bins = 30)#ploting histogram
plt.show()

# function to build a tokenizer
def tokenization(lines):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(lines)
    return tokenizer

# prepare english tokenizer
eng_tokenizer = tokenization(ger_eng[:, 0])
eng_vocab_size = len(eng_tokenizer.word_index) + 1
eng_length = 8

#german tokenizer
ger_tokenizer = tokenization(ger_eng[:,1])
ger_vocab_size = len(ger_tokenizer.word_index)+1
ger_length = 8

# encode and pad sequences
def encode_sequences(tokenizer, length, lines):
         # integer encode sequences
         seq = tokenizer.texts_to_sequences(lines)
         # pad sequences with 0 values
         seq = pad_sequences(seq, maxlen=length, padding='post')
         return seq

#Model Building
#We will now split the data into train and test set for model training and evaluation, respectively
from sklearn.model_selection import train_test_split

# split data into train and test set
train, test  = train_test_split(ger_eng, test_size=0.2, random_state = 12)

#We will encode German sentences as the input sequences and English sentences as the target sequences. 
# prepare training data
X_train = encode_sequences(ger_tokenizer, ger_length, train[:, 1])
Y_train = encode_sequences(eng_tokenizer, eng_length, train[:, 0])

# prepare validation data
X_test = encode_sequences(ger_tokenizer, ger_length, test[:, 1])
Y_test = encode_sequences(eng_tokenizer, eng_length, test[:, 0])



# build NMT model
def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):
      model = Sequential()
      model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))
      model.add(LSTM(units))
      model.add(RepeatVector(out_timesteps))
      model.add(LSTM(units, return_sequences=True))
      model.add(Dense(out_vocab, activation='softmax'))
      return model

# model compilation
model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 512)

rms = optimizers.RMSprop(lr=0.001)
model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')



filename = 'translation_model'
checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')

# train model
history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1),
                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], 
                    verbose=1)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['train','validation'])
plt.show()

#LOAD MODEL
model = load_model('translation_model')
preds = model.predict_classes(X_test.reshape((X_test.shape[0],X_test.shape[1])))

def get_word(n, tokenizer):
      for word, index in tokenizer.word_index.items():
          if index == n:
              return word
      return None

#Convert predictions into text (English):
preds_text = []
for i in preds:
    temp = []
    for j in range(len(i)):
        t = get_word(i[j], eng_tokenizer)
        if j > 0:
            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):
                    temp.append('')
            else:
                    temp.append(t)
        else:
                if(t == None):
                        temp.append('')
                else:
                        temp.append(t) 

    preds_text.append(' '.join(temp))

#Letâ€™s put the original English sentences in the test dataset and the predicted sentences in a dataframe:
pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})



# print 15 rows randomly
pred_df.sample(15)

